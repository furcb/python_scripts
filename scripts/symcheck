#!/bin/env python

#TODO: Add keep alive option for cases where symcheck fails to write checksum file, gives user
        # ability to fix whatever issue is preventing the script to write the file with a loop
        # asking user input if symcheck should retry or exit
#TODO: Add ability to also check single files. Useful when you want to check binary files and diff
        # is not appropriate

"""Similarity checker

Checks two directories which are supposed to be copies and outputs dissimilar files. Useful for
checking file integrity after transferring between devices"

Usage:
    symcheck [options] <dir> <copy_dir>...

Arguments:
    <dir>       Original directory that files were copied from.
    <copy_dir>  New directories

Options:
    -s, --save-original     Save checksums from <dir> to checksum.{type} in <copy_dir> root.
    -x, --save-copies       Save checksums from <copy_dir> to checksum.{type} in <copy_dir> root.
    -a, --check-hidden      Checks hidden files as well, default is to ignore .dirs and .files
    --show-all              Shows passes and fails, default is to show failed checks only.

Additional notes:

The normal use case is to copy over the original checksums. This means if both s and x flags are
provided, originals will take precedence.
"""

import os, sys, glob
from os import path as pt
from docopt import docopt

import pprint, hashlib
from pathlib import PurePath

# error message
def eprint(msg):
    print(f"{msg}", file=sys.stderr)

# Compares original directory and copies
def compare(og, cps):
    results = {}

    og_key = list(og.keys())[0]
    og_tmp = set(list(zip(og[og_key]["paths"], og[og_key]["hashes"])))

    for path_name, values in cps.items():
        tmp_ziplist = list(zip(values["paths"], values["hashes"]))

        results[path_name] = {}
        results[path_name]["similar"] = sorted(og_tmp.intersection(tmp_ziplist), key=lambda a:a[0])
        results[path_name]["dissimilar"] = sorted(og_tmp.difference(tmp_ziplist), key=lambda a:a[0])

    return results

# Generates hashes for files in dir, expects dict of path: [ file list ]
def generate_hashes(dirs):
    for path_name, values in dirs.items(): # go through all dirs
        dirs[path_name]["hashes"] = []
        total = len(values["paths"])
        current = 0
        print(f"Generating checksums for {path_name}:")
        for fpath in values["paths"]:      # go through all files
            try:
                with open(pt.join(path_name, fpath), "rb") as fb:
                    hgen = hashlib.sha256()
                    # needed for when a file is larger than memory capicity of the system
                    # read files in chunks of 65KB
                    while chunk := fb.read(65536):
                        hgen.update(chunk)
                    dirs[path_name]["hashes"].append(hgen.hexdigest())
                    fb.close()
            except Exception as e:
                eprint(f"Failed when opening and generating a hash for {fpath}, exiting.\nError: \
                {e}")
            current += 1
            print_progress(current, total)
        # New line for better readability
        print()

# Checks if dirs exists. Exits on error or if check fails
def dirs_exist(paths):
    for p in paths:
        try:
            if not pt.isdir(p):
                eprint(f"{p} is not a directory, exiting")
                sys.exit(1)
        except Exception as e:
            eprint(f"Unable to check if {p} is a directory, due to \n{e}")
            sys.exit(1)

# Makes path list relative to start
def make_relative(start, paths):
    new_paths = []
    for p in paths:
        new_paths.append(pt.relpath(p, start=start))

    return new_paths

# Prints results by cp directory
def print_results(results, show_similar, show_dissimilar):
    for path_name, v in results.items():
        print(f"\n== Showing results for {path_name} ==")
        if show_similar:
            for f in v["similar"]:
                print(f"Checksum: pass: {f[0]}")

        if show_dissimilar:
            for f in v["dissimilar"]:
                print(f"Checksum: fail: {f[0]}")

# Prints percent progress of process
def print_progress(current, total):
    percent = current / total
    total = str(total)
    current = str(current).zfill(len(total))
    print(f"\r{current}/{total} : {int(percent*100)}%", end="")

# Returns a dictionary with lists of files with path as key
# { "path_name": { paths: ["path/to/file", ...] }, ...}
def get_file_list(paths, search_hidden):
    file_list = {}
    for p in paths:
        try:
            print(f"Finding files in {p}")
            rel_list = []
            if search_hidden:
                for root, dirs, files in os.walk(p):
                    for fname in files:
                        rel_list.append(pt.join(root, fname))
                rel_list = list(filter(lambda x: pt.isfile(x), rel_list))
                rel_list = list(map(lambda x: PurePath(x).relative_to(p).as_posix(), rel_list))
            else:
                rel_list = glob.glob(f"{p}/**/*", recursive=True)
                rel_list = list(filter(lambda x: pt.isfile(x), rel_list))
                rel_list = make_relative(p, rel_list)

            file_list[p] = { "paths": rel_list }
        except Exception as e:
            eprint(f"Failed to get file list for {p} due to: \n{e}")
            sys.exit(1)
    return file_list

# Saves checksums to a valid checksum file
def save_checksums(paths, values):
    for path_name in paths:
        file_path = pt.join(path_name, "checksums.sha256")
        try:
            with open(file_path, "x") as f:
                file_data = zip(values["hashes"], values["paths"])
                file_data = list(map(lambda x: f"{x[0]} {x[1]}\n", file_data))
                f.write("".join(file_data))
                f.close()
        except Exception as e:
            eprint(f"Failed to save checksums to {file_path} due to this error:\n{e}")

def main(args):
    # get dirs
    og_dir = args["<dir>"]
    cp_dirs = args["<copy_dir>"]

    # check if they exist
    dirs_exist([og_dir])
    dirs_exist(cp_dirs)

    # generate file lists for both
    og_files = get_file_list([og_dir], args["--check-hidden"])
    cp_files = get_file_list(cp_dirs, args["--check-hidden"])

    # generate file hashes for both
    generate_hashes(og_files)
    generate_hashes(cp_files)

    # check for inconsistent data
    results = compare(og_files, cp_files)
    
    if args["--show-all"]:
        print_results(results, True, True)
    else:
        print_results(results, False, True)

    # create hash file and save hash data
    cp_paths = cp_files.keys()
    values = og_files[list(og_files.keys())[0]]
    if args["--save-original"]:
        save_checksums(cp_paths, values)
    elif args["--save-copies"]:
        save_checksums(cp_paths, values)


if __name__ == "__main__":
    args = docopt(__doc__)
    main(args)
